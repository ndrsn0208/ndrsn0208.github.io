---
layout: about
title: about
permalink: /
subtitle: <a href='https://sled.eecs.umich.edu/author/zekun-anderson-wang/'>University of Michigan</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>2260 Hayward</p>
    <p>Ann Arbor, MI 48103</p>

news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
**About Me**

I'm currently a second-year M.S. student in computer science at the Univerity of Michigan. I'm advised by [Dr. Joyce Chai](https://web.eecs.umich.edu/~chaijy/) and [Dr. Rada Mihalcea](https://web.eecs.umich.edu/~mihalcea/). My current projects involve building a computational model of child language acquisition from the idea of interactive learning and imitation learning as well as analyzing abstract and concrete concepts understanding in grounded/ungrounded systems. My deepest research interest is rooted in exploring human cognition and how we "learn so much from so little". My experiences in *NLP*, *CV*, *Reinforcement Learning* and *Cognition modeling* sculptured my commitment to blending cognitive principles to develop "learning" methods instead of "modeling" methods.

**Research Interests**

I am deeply interested in the way we, particularly children, acquire and derive novel knowledge from past learned experiences (limited data) and then transfer far. Such learning scheme posts a stark contrast to the current machine learning paradigm, which is data-hungry and "modeling", where "learning" is an active, interactive, efficient, adaptive and creative process. As a result, I'm interested in exploring the following fundemental questions:
- **Compositionality**: One of the key component that I deemed essential that enabled us to learn fast and be creative. My hypothesis is that our mind creates heuristics to search and compose past knowledge (or parts of) to create the best fit for current understanding. Consider the example of learning math. In principle, if one knows how to perform elementary arithmetic, then one should can derive calculus, linear algebra, and so on. However, such derivation may never arrive without proper "kicks", or the right heuristics. I'm interested 1. how we can develop a compositional memory and 2. how we can learn the best "kicks" for knowledge composition.
- **Continual Learning**: A natural extension of compositionality. In the real world, we are constantly learning new things and at the same time we forget things. However, that doesn't hugely impact on how we utilize old knowledge and acquire new knowledge. Theories in replay buffer, memory consolidation, and catastrophic forgetting suggest that our learning is not linear and might be stochastic.
- **Active Learning**: Goal-oriented agents like human creates unique objectives for different needs. This requires the agent to 1. derive desires and 2. be able to plan subgoals to achieve the desires. My current apporach would be framing such problem in the contetx of reinforcement learning and inverse reinforcement learning.
- **Meta-Learning**: Finally, meta-learning is the general underlining principle that enables all of the above. Human will learn how to learn math by adjusting their leanring behavior through practice and experience, and learn what past knowledge to use and what to ignore.