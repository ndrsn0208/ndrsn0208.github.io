---
---

@string{aps = {American Physical Society,}}

@inproceedings{wangtaxonnet2025,
  abbr = {NeurIPS 2025},
  author = {Zekun Wang and Ethan Haarer and Tianyi Zhu and Zhiyi Dai and Christopher MacLellan},
  title = {Deep Taxonomic Networks for Unsupervised Hierarchical Prototype Discovery},
  year = {2025},
  selected={true}

}

@inproceedings{wangacs2025,
  abbr = {ACS 2025},
  author = {Anant Gupta and Karthik Singaravadivelan and Zekun Wang and Christopher MacLellan},
  title = {Hierarchical Semantic Retrieval with Cobweb},
  year = {2025},
  selected={true}
}

@inproceedings{wangneus2025,
  abbr = {NeuS 2025},
  author = {Zekun Wang and Ethan Haarer and Nicki Barari and Christopher MacLellan},
  title = {Taxonomic Networks: A Representation for Neuro-Symbolic Pairing},
  year = {2025},
  selected={true}
}

@inproceedings{wangcvgt2025,
  abbr = {CogSci 2025},
  author = {Zekun Wang and Sashank Varma},
  title = {Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts},
  year = {2025},
  selected={true}
}

@inproceedings{ma-etal-2025-babysit,
    abbr = {NAACL 2025},
    title = "Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations",
    author = "Ma, Ziqiao  and
      Wang, Zekun  and
      Chai, Joyce",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.46/",
    doi = "10.18653/v1/2025.naacl-long.46",
    pages = "991--1010",
    ISBN = "979-8-89176-189-6",
    selected={true},
    abstract = "Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contributes to word learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher{'}s choices of words influence students' word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and active trials, can facilitate efficient word learning in language models."
}

@unpublished{zekun2023interactive,
  abbr = {ICML Workshop},
  author = {Ziqiao Ma and Zekun Wang and Joyce Chai},
  title  = {Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations},
  year   = {2024}
}

@misc{ignat2023phd,
      abbr={LREC-COLING},
      title={Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models}, 
      author={Oana Ignat and Zhijing Jin and Artem Abzaliev and Laura Biester and Santiago Castro and Naihao Deng and Xinyi Gao and Aylin Gunal and Jacky He and Ashkan Kazemi and Muhammad Khalifa and Namho Koh and Andrew Lee and Siyang Liu and Do June Min and Shinka Mori and Joan Nwatu and Veronica Perez-Rosas and Siqi Shen and Zekun Wang and Winston Wu and Rada Mihalcea},
      year={2024},
      selected={true}
}

@inproceedings{Lloyd_2022, series={ICOTS 11},
  abbr={ICOTS},
   title={Foundations for AI-Assisted Formative Assessment Feedback for Short-Answer Tasks in Large-Enrollment Classes},
   url={http://dx.doi.org/10.52041/iase.icots11.T3C3},
   DOI={10.52041/iase.icots11.t3c3},
   booktitle={Bridging the Gap: Empowering and Educating Today's Learners in Statistics. Proceedings of the Eleventh International Conference on Teaching Statistics},
   publisher={International Association for Statistical Education},
   author={Lloyd, Susan and Beckman, Matthew and Pearl, Dennis and Passonneau, Rebecca and Li, Zhaohui and Wang, Zekun},
   year={2022},
   month=dec, collection={ICOTS 11} ,
   selected={true}
   }